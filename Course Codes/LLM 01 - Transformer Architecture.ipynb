{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e81e8a3f-9fcf-4381-8aed-1a1487206193",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a32854-da61-4498-af67-df191973c58a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Diving into the Transformer - Building your own Foundation LLM\n",
    "\n",
    "This lesson introduces the underlying sturcture of transformers from token management to the layers in a decoder, to comparing smaller and larger models. We will build up all of the steps needed to create our foundation model before training. You will see how the layers are constructed, and how the next word is chosen. \n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Identify the key structures and functions in decoder transformers\n",
    "1. Analyze the effect of hyperparameter changes (such as embedding dimension) on the size of the LLM\n",
    "1. Compare the different performance of models with different model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "785e356c-f1f9-4f16-9e41-e6223cac67e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d46c46-24d2-4857-8913-e40bd9c232b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cf93fc7-4bec-4249-9506-52e99ce8613b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d181e39e-6ead-40eb-b500-8f7fc5fb3ac6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 1: Encoding Natural Language - Word Embedding and Positional Encoding\n",
    "\n",
    "In this section we'll look at how to take a natural language input and convert it to the form we'll need for our transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7d6c2c9-99de-4e81-abf1-909f239ea19d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a sentence and a simple word2id mapping\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "word2id = {word: i for i, word in enumerate(set(sentence.split()))}\n",
    "print(word2id)\n",
    "# Convert text to indices\n",
    "input_ids = torch.tensor([word2id[word] for word in sentence.split()])\n",
    "print(input_ids)\n",
    "# Define a simple word embedding function\n",
    "def get_word_embeddings(input_ids, embedding_size):\n",
    "    embedding_layer = nn.Embedding(input_ids.max() + 1, embedding_size)\n",
    "    return embedding_layer(input_ids)\n",
    "\n",
    "\n",
    "# Get word embeddings\n",
    "embedding_size = 16  # Size of the word embeddings\n",
    "word_embeddings = get_word_embeddings(input_ids, embedding_size)\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c228cf8c-48fe-4152-ab13-350be0aa6c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to generate positional encodings\n",
    "def get_positional_encoding(max_seq_len, d_model):\n",
    "    position = np.arange(max_seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    positional_encoding = np.zeros((max_seq_len, d_model))\n",
    "    positional_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "    positional_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "    return torch.tensor(positional_encoding, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2d4ef8a-d019-4172-8b14-a04b6e1c0d61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to plot heatmap\n",
    "# ------------------------\n",
    "def plot_heatmap(data, title):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    seaborn.heatmap(data, cmap=\"cool\",vmin=-1, vmax=1)\n",
    "    plt.ylabel(\"Word/token\")\n",
    "    plt.xlabel(\"Positional Encoding Vector\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Generate and plot positional encoding\n",
    "# -------------------------------------\n",
    "# Get positional encodings\n",
    "max_seq_len = len(sentence.split())  # Maximum sequence length\n",
    "d_model = embedding_size  # Same as the size of the word embeddings\n",
    "positional_encodings = get_positional_encoding(max_seq_len, d_model)\n",
    "plot_heatmap(positional_encodings, \"Positional Encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c46f50-b2a3-4769-b821-1b4ec6c8a376",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Interpreting the Positional Encoding Map\n",
    "In the Transformer model, positional encoding is used to give the model some information about the relative positions of the words in the sequence since the Transformer does not have any inherent sense of order of the input sequence.\n",
    "\n",
    "The positional encoding for a position \\(p\\) in the sequence and a dimension \\(i\\) in the embedding space is a mix of sine and cosine functions:\n",
    "\n",
    "\n",
    "$$PE_{(p, 2i)} = \\sin\\left(\\frac{p}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "\n",
    "\n",
    "$$PE_{(p, 2i+1)} = \\cos\\left(\\frac{p}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "\n",
    "Here, \\\\(d\\\\) is the dimension of the word embedding.\n",
    "\n",
    "These functions were chosen because they can provide a unique encoding for each word position and these encodings can be easily learned and extrapolated for sequence lengths not seen during training.\n",
    "\n",
    "In the heatmap:\n",
    "\n",
    "- The x-axis represents the dimension of the embedding space. Every pair of dimensions \\\\((2i, 2i+1)\\\\) corresponds to a specific frequency of the sine and cosine functions.\n",
    "\n",
    "- The y-axis represents the position of a word in the sequence.\n",
    "\n",
    "- The color at each point in the heatmap represents the value of the positional encoding at that position and dimension. Typically, a warmer color (like red) represents a higher value and a cooler color (like blue) represents a lower value.\n",
    "\n",
    "By visualizing the positional encodings in a heatmap, we can see how these values change across positions and dimensions, and get an intuition for how the Transformer model might use these values to understand the order of words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c620a17-a0dc-4b21-b22d-e3d8583c58c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get positional encodings\n",
    "max_seq_len = len(sentence.split())  # Maximum sequence length\n",
    "d_model = embedding_size  # Same as the size of the word embeddings\n",
    "positional_encodings = get_positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "# Add word embeddings and positional encodings\n",
    "final_embeddings = word_embeddings + positional_encodings\n",
    "\n",
    "print(final_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "272e180e-6fc3-4d91-a5ac-ac2bb2b13842",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 2: Building Our Own Decoder From Scratch\n",
    "\n",
    "Let's now build a decoder transfomer. We'll build up the code from scratch and build a single layer transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "511fdcd1-2ac5-41d0-b579-8b1a6f3ffd68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here we define the DecoderBlock, which is a single layer of the Transformer Decoder.\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_hidden_dim, dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "    # The first part of the __init__ function defines the hyperparameters for the DecoderBlock.\n",
    "    # d_model: the dimension of the input vector.\n",
    "    # num_heads: the number of heads in the multihead attention mechanism.\n",
    "    # ff_hidden_dim: the dimension of the feed forward hidden layer.\n",
    "    # dropout: the dropout rate.\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(d_model, ff_hidden_dim)\n",
    "        self.linear2 = nn.Linear(ff_hidden_dim, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    # The forward method defines how the data flows through the network.\n",
    "    # It takes two inputs: x, tgt_mask.\n",
    "    # x: the input tensor.\n",
    "    # tgt_mask: masks to prevent attention to certain positions.\n",
    "\n",
    "    def forward(self, x,tgt_mask):\n",
    "        attn_output, _ = self.self_attention(x, x, x, attn_mask=tgt_mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        ff_output = self.linear2(F.relu(self.linear1(x)))\n",
    "        x = x + self.dropout2(ff_output)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6946186b-74ce-4c97-ac70-2d146a3af558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Next, we define the PositionalEncoding class, which applies a specific positional encoding to give the model \n",
    "# information about the relative or absolute position of the tokens in the sequence.\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76bb2424-307d-49d6-9fb7-5aed36a2ec86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Finally, we define the full Transformer Decoder, which includes the initial embedding layer, \n",
    "# a single Transformer Decoder block, and the final linear and softmax layers.\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, ff_hidden_dim, dropout):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "    # The __init__ function defines the hyperparameters and layers of the TransformerDecoder.\n",
    "    # vocab_size: the size of the vocabulary.\n",
    "    # d_model, num_heads, ff_hidden_dim, dropout: hyperparameters for the Transformer decoder block.\n",
    "\n",
    "    # Embedding layer: transforms the input words (given as indices) into dense vectors of dimension d_model.\n",
    "    # Positional encoding: adds a vector to each input embedding that depends on its position in the sequence.\n",
    "    # Transformer block: the Transformer decoder block defined earlier.\n",
    "    # Linear layer: a linear transformation to the output dimension equal to the vocabulary size.\n",
    "    # Softmax layer: transforms the output into a probability distribution over the vocabulary.\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer_block = DecoderBlock(d_model, num_heads, ff_hidden_dim, dropout)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    # The forward method of the TransformerDecoder defines how the data flows through the decoder.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        tgt_mask = generate_square_subsequent_mask(x.size(0))\n",
    "        x = self.transformer_block(x,tgt_mask)\n",
    "        output = self.linear(x)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d5ff14-09ce-4eaa-a114-db5cd06bb672",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Why we need to mask our input for decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e50e693-14b2-4de0-b12b-ab96fad0cdaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate a mask to prevent attention to future positions.\"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "mask = generate_square_subsequent_mask(sz=5)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "seaborn.heatmap(mask, cmap=\"viridis\", cbar=False, square=True)\n",
    "plt.title(\"Mask for Transformer Decoder\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28cdcb9e-15dc-42fc-a806-c8114dc14fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Let's make our first decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19edf35a-a3bf-4c83-98b8-6c29524494fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "vocab_size     = 1000\n",
    "d_model        = 512\n",
    "num_heads      = 1\n",
    "ff_hidden_dim  = 2*d_model\n",
    "dropout        = 0.1\n",
    "num_layers     = 10\n",
    "context_length = 50\n",
    "batch_size     = 1\n",
    "# Initialize the model\n",
    "model = TransformerDecoder(vocab_size, d_model, num_heads, ff_hidden_dim, dropout)\n",
    "\n",
    "# Create a tensor representing a batch of 1 sequences of length 10\n",
    "input_tensor = torch.randint(0, vocab_size, (context_length, batch_size))\n",
    "\n",
    "# Forward pass through the model\n",
    "output = model(input_tensor)\n",
    "\n",
    "# The output is a tensor of shape (sequence_length, batch_size, vocab_size)\n",
    "print(output.shape)  # Should print torch.Size([context_length, batch_size, vocab_size])\n",
    "\n",
    "# To get the predicted word indices, we can use the `argmax` function\n",
    "predicted_indices = output.argmax(dim=-1)\n",
    "\n",
    "# Now `predicted_indices` is a tensor of shape (sequence_length, batch_size) containing the predicted word indices\n",
    "print(predicted_indices.shape)  # Should print torch.Size([context_length, batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "252591bf-b9bb-416a-b9a0-e40704ce13d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec5e5c5b-d930-4602-ae4e-4f0559936e68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Looking at the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7363e64-3556-42e3-8c0a-36deb646eb43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the log probabilities to probabilities\n",
    "distribution = torch.exp(output[0, 0, :])\n",
    "\n",
    "# Convert the output tensor to numpy array\n",
    "distribution = distribution.detach().numpy()\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(np.arange(vocab_size), distribution)\n",
    "plt.xlabel(\"Word Index\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Output Distribution over Vocabulary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "043ea40d-51a4-4ccd-aabd-8122d7f42484",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 3: Multi-layer Decoder\n",
    "\n",
    "Let's allow for multiple layers in our decoder so we can form models like GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fada583-5535-4d8b-8f0b-279853b1086c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MultiLayerTransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, ff_hidden_dim, dropout, num_layers):\n",
    "        super(MultiLayerTransformerDecoder, self).__init__()\n",
    "\n",
    "# The __init__ function now also takes a `num_layers` argument, which specifies the number of decoder blocks.\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, ff_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "# The forward method has been updated to pass the input through each transformer block in sequence.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            tgt_mask = generate_square_subsequent_mask(x.size(0))\n",
    "            x = transformer_block(x,tgt_mask)\n",
    "        output = self.linear(x)\n",
    "        output = self.softmax(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cdd1093-a7d7-4962-85ec-104adb6e3cbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "vocab_size     = 10000\n",
    "d_model        = 2048\n",
    "num_heads      = 1\n",
    "ff_hidden_dim  = 4*d_model\n",
    "dropout        = 0.1\n",
    "num_layers     = 10\n",
    "context_length = 100\n",
    "batch_size     = 1\n",
    "\n",
    "# Create our input to the model to process\n",
    "input_tensor = torch.randint(0, vocab_size, (context_length, batch_size))\n",
    "\n",
    "# Initialize the model with `num_layer` layers\n",
    "model = MultiLayerTransformerDecoder(vocab_size, d_model, num_heads, ff_hidden_dim, dropout, num_layers)\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "\n",
    "# Let's use the same input_tensor from the previous example\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Convert the log probabilities to probabilities for the first sequence in the batch and the first position in the sequence\n",
    "distribution = torch.exp(output[0, 0, :])\n",
    "\n",
    "# Convert the output tensor to numpy array\n",
    "distribution = distribution.detach().numpy()\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(np.arange(vocab_size), distribution)\n",
    "plt.xlabel(\"Word Index\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Output Distribution over Vocabulary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f173612-53bc-4377-9a0f-f7edd5241aff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2539ff71-a836-4cad-8010-ad9c7e060218",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 4: Adding real vocabulary to our model\n",
    "\n",
    "Rather than just using a random integer, let's add in a small vocabulary of real words and let our model speak!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae34cf0f-551f-4b91-9074-045379f82181",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "d_model        = 100\n",
    "num_heads      = 1\n",
    "ff_hidden_dim  = 4*d_model\n",
    "dropout        = 0.1\n",
    "num_layers     = 4\n",
    "context_length = 5\n",
    "batch_size     = 1\n",
    "# Define the vocabulary\n",
    "vocab = [\"of\", \"in\", \"to\", \"for\", \"with\", \"on\", \"at\", \"from\", \"by\", \"about\", \"as\", \"into\", \"like\", \"through\", \"after\", \"over\", \"between\", \"out\", \"against\", \"during\", \"without\", \"before\", \"under\", \"around\", \"among\"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create a dictionary that maps words to indices\n",
    "word2id = {word: id for id, word in enumerate(vocab)}\n",
    "\n",
    "# Create a dictionary that maps indices to words\n",
    "id2word = {id: word for id, word in enumerate(vocab)}\n",
    "\n",
    "# Initialize the model\n",
    "model = MultiLayerTransformerDecoder(vocab_size, d_model, num_heads, ff_hidden_dim, dropout, num_layers)\n",
    "\n",
    "# Create a tensor representing a single sequence of variable length\n",
    "# Here we randomly select words from our vocabulary\n",
    "sequence = [\"of\", \"in\", \"to\", \"for\", \"with\", \"on\", \"at\"][:context_length]\n",
    "input_tensor = torch.tensor([[word2id[word] for word in sequence]])\n",
    "\n",
    "# Generate a sequence of words\n",
    "generated_words = []\n",
    "for i in range(10):  # Generate 10 words\n",
    "    output = model(input_tensor)\n",
    "    predicted_index = output.argmax(dim=-1)[0, -1]  # Take the last word in the sequence\n",
    "    predicted_word = id2word[predicted_index.item()]\n",
    "    print(predicted_word, end=\" \")\n",
    "    generated_words.append(predicted_word)\n",
    "    input_tensor = torch.cat([input_tensor, predicted_index.unsqueeze(0).unsqueeze(0)], dim=-1)  # Append the predicted word to the input\n",
    "    time.sleep(0.75)  # Pause for 1 second\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7be5d59-6650-45dd-b262-325c60184f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 5: Using a trained decoder and real-world vocabulary\n",
    "\n",
    "Training our model will take a long time, let's look at two trained versions of what we've been building, GPT and GPT-XL. These are both decoder models with only slight changes in sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d25a8f91-cbc9-4909-924b-bbd88559b31b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained models and tokenizers\n",
    "tokenizer_small = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "model_small = GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir=DA.paths.datasets+\"/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ad9c89a-ffd5-47ce-8fd8-062209f1dcbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a prompt. This is the initial string of text that the model will use to start generating text.\n",
    "prompt = \"This is a MOOC about large language models, I have only just started, but already\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21cf980f-ac62-4c30-9e46-ce0d44d7ed14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We use the tokenizer to convert the prompt into a format that the model can understand. In this case,\n",
    "# it converts the string into a sequence of token IDs, which are numbers that represent each word or subword in the string.\n",
    "inputs_small = tokenizer_small.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Create an attention mask. This is a sequence of 1s and 0s where 1s indicate that the corresponding token should\n",
    "# be attended to and 0s indicate that the token should be ignored. Here, all tokens should be attended to.\n",
    "attention_mask_small = torch.ones(inputs_small.shape, dtype=torch.long)\n",
    "\n",
    "# Get the ID of the special end-of-sequence (EOS) token from the tokenizer. This token indicates the end of a sequence.\n",
    "pad_token_id_small = tokenizer_small.eos_token_id  \n",
    "\n",
    "# Print the initial prompt. The 'end' argument specifies what to print at the end (default is newline, but we want space).\n",
    "# 'flush' argument ensures that the output is printed immediately.\n",
    "print(prompt, end=\" \", flush=True)\n",
    "\n",
    "# We're going to generate 25 words\n",
    "for _ in range(25):\n",
    "\n",
    "    # Generate the next part of the sequence. 'do_sample=True' means to sample from the distribution of possible next tokens\n",
    "    # rather than just taking the most likely next token. 'pad_token_id' argument is to tell the model what token to use if it\n",
    "    # needs to pad the sequence to a certain length.\n",
    "    outputs_small = model_small.generate(inputs_small, max_length=inputs_small.shape[-1]+1, do_sample=True, pad_token_id=pad_token_id_small,\n",
    "                                         attention_mask=attention_mask_small)\n",
    "\n",
    "    # The generated output is a sequence of token IDs, so we use the tokenizer to convert these back into words.\n",
    "    generated_word = tokenizer_small.decode(outputs_small[0][-1])\n",
    "\n",
    "    # Print the generated word, followed by a space. We use 'end' and 'flush' arguments as before.\n",
    "    print(generated_word, end=' ', flush=True)\n",
    "\n",
    "    # Append the generated token to the input sequence for the next round of generation. We have to add extra dimensions \n",
    "    # to the tensor to match the shape of the input tensor (which is 2D: batch size x sequence length).\n",
    "    inputs_small = torch.cat([inputs_small, outputs_small[0][-1].unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "    # Extend the attention mask for the new token. Like before, it should be attended to, so we add a 1.\n",
    "    attention_mask_small = torch.cat([attention_mask_small, torch.ones((1, 1), dtype=torch.long)], dim=-1)\n",
    "\n",
    "    # We pause for 0.7 seconds to make the generation more readable.\n",
    "    time.sleep(0.7)\n",
    "\n",
    "# Finally, print a newline and a completion message.\n",
    "print(\"\\nGPT-2 Small completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "173e7887-b620-4790-9b9a-649f61f3a428",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_large = GPT2Tokenizer.from_pretrained(\"gpt2-XL\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "model_large = GPT2LMHeadModel.from_pretrained(\"gpt2-XL\", cache_dir=DA.paths.datasets+\"/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad05398b-1813-4ba8-acd2-6a6f79cd216d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate text with GPT-2 XL\n",
    "inputs_large = tokenizer_large.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Add in the attention mask and pad token id\n",
    "attention_mask_large = torch.ones(inputs_large.shape, dtype=torch.long)  # Creating a mask of ones with the same shape as inputs\n",
    "pad_token_id_large = tokenizer_large.eos_token_id  # Get the eos_token_id from the tokenizer\n",
    "\n",
    "print(prompt, end=\" \", flush=True)\n",
    "for _ in range(25):  # Generate 25 words\n",
    "    outputs_large = model_large.generate(inputs_large, max_length=inputs_large.shape[-1]+1, do_sample=True, pad_token_id=pad_token_id_large,\n",
    "                                         attention_mask=attention_mask_large)\n",
    "    generated_word = tokenizer_large.decode(outputs_large[0][-1])\n",
    "    print(generated_word, end=\" \", flush=True)\n",
    "    inputs_large = torch.cat([inputs_large, outputs_large[0][-1].unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "    attention_mask_large = torch.cat([attention_mask_large, torch.ones((1, 1), dtype=torch.long)], dim=-1)\n",
    "    time.sleep(0.7)\n",
    "print(\"\\nGPT-2 XL completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "876615b9-6e99-40e0-94bc-5d4437961c41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM 01 - Transformer Architecture",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
